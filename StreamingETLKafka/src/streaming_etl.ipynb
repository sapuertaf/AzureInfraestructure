{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#globales a usar en el programa\n",
    "RAW_PATH = \"wasbs://raw@storagesergio9090.blob.core.windows.net\"\n",
    "MOUNT_POINT_RAW = \"/mnt/raw\"\n",
    "RAW_CHECKPOINT_LOCATION = \"/mnt/raw/check\"\n",
    "RAW_DATA_PATH = \"/mnt/raw/test-data\"\n",
    "SILVER_PATH = \"wasbs://silver@storagesergio9090.blob.core.windows.net\"\n",
    "MOUNT_POINT_SILVER = \"/mnt/silver\"\n",
    "SILVER_CHECKPOINT_LOCATION = \"/mnt/silver/check\"\n",
    "SILVER_DATA_PATH = \"/mnt/silver/test-data\"\n",
    "DATABRICKS_SCOPE = \"databricks-secret-scope1\"\n",
    "AZURE_SECRET_PATH = \"fs.azure.account.key.storagesergio9090.blob.core.windows.net\"\n",
    "AZURE_SECRET_NAME = \"data-lake-access-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurando y montando puntos de montaje raw DataBricks - Azure\n",
    "dbutils.fs.mount(\n",
    "    source = RAW_PATH,\n",
    "    mount_point = MOUNT_POINT_RAW,\n",
    "    extra_configs = {AZURE_SECRET_PATH:dbutils.secrets.get(scope=DATABRICKS_SCOPE, key=AZURE_SECRET_NAME)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurando y montando puntos de montaje silver DataBricks - Azure\n",
    "dbutils.fs.mount(\n",
    "    source = SILVER_PATH,\n",
    "    mount_point = MOUNT_POINT_SILVER,\n",
    "    extra_configs = {AZURE_SECRET_PATH:dbutils.secrets.get(scope=DATABRICKS_SCOPE, key=AZURE_SECRET_NAME)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, StructType, StringType, TimestampType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el esquema para los datos que son publicado en el tópico de Kafka (se le da una estructura a los mensajes de entrada)\n",
    "schema = StructType()\\\n",
    "         .add(\"timestamp\", TimestampType())\\\n",
    "         .add(\"url\", StringType())\\\n",
    "         .add(\"userURL\", StringType())\\\n",
    "         .add(\"pageURL\", StringType())\\\n",
    "         .add(\"isNewPage\", BooleanType())\\\n",
    "         .add(\"geocoding\", StructType()\n",
    "            .add(\"countryCode2\", StringType())\n",
    "            .add(\"city\", StringType())\n",
    "            .add(\"latitude\", StringType())\n",
    "            .add(\"country\", StringType())\n",
    "            .add(\"longitude\", StringType())\n",
    "            .add(\"stateProvince\", StringType())\n",
    "            .add(\"countryCode3\", StringType())\n",
    "            .add(\"user\", StringType())\n",
    "            .add(\"namespace\", StringType())\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comando para cargar datos de cualquier tipo de fuente. Se utiliza el objeto Spark que permite la conexión hacia el cluster de Spark. (transformación: leer o suscribirse a un canal de kafka)\n",
    "kafkaDF = (spark   # kafkaDF es un DataFrame en un objeto de python.\n",
    "           .readStream # Función para conexión a un cluster de Kafka\n",
    "           .option(\"kafka.bootstrap.servers\", \"server1.databricks.training:9092\") # Servidor público para pruebas\n",
    "           .option(\"subscribe\", \"en\") # Sucripción a un canal con el nombre de \"en\"\n",
    "           .format(\"kafka\")  # Se define el formato tipo kafka\n",
    "           .load() \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los DataFrame de Spark son inmutables, una vez creados no se pueden modificar (se debe sobreescribir o crear uno nuevo)\n",
    "kafkaCleanDF = (kafkaDF\n",
    "                .select(from_json(col(\"value\").cast(StringType()),schema).alias(\"message\")) # Ingresando dentro de value, transformar los datos (Type), se aplica el esquema desarrollado anteriormente y se le pone un alias con el nombre de \"message\"\n",
    "                .select(\"message.*\") # traer todas las columnas de la variable \"mmessage\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myStreamName = \"prueba_streaming\"\n",
    "display(kafkaCleanDF, streamName = myStreamName) # Crea una tabla en donde se pueden observar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar el Stream e inicializandolo con la transformación realizada\n",
    "# Se empezará a aplicar transformaciones a los datos en tiempo real. Se analizarán sólo los datos de los paises no nulos.\n",
    "\n",
    "geocodingDF = (kafkaCleanDF\n",
    "              .filter(col(\"geocoding.country\").isNotNull()) # Se ingresa a la columna \"geocoding_country\" y se seleccionan las filas con este atributo no nulo\n",
    "              .select(\"timestamp\", \"pageURL\", \"geocoding.countryCode2\", \"geocoding.city\") # Me quedo con los atribudos que me importan para el análisis\n",
    "              )\n",
    "display(geocodingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en capa raw (bronze)\n",
    "(spark.readStream\n",
    "  .format(\"kafka\")  \n",
    "  .option(\"kafka.bootstrap.servers\", \"server1.databricks.training:9092\")\n",
    "  .option(\"subscribe\", \"en, ja\")\n",
    "  .load()\n",
    "  .withColumn(\"json\", from_json(col(\"value\").cast(\"string\"), schema))\n",
    "  .select(col(\"timestamp\").alias(\"KafkaTimestamp\"), col(\"json.*\"))\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", RAW_CHECKPOINT_LOCATION)\n",
    "  .outputMode(\"append\")\n",
    "  .queryName('bronze_stream')\n",
    "  .start(RAW_DATA_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en capa silver\n",
    "(spark.readStream\n",
    "  .format(\"delta\")\n",
    "  .load(RAW_DATA_PATH)\n",
    "  .select(col('KafkaTimestamp'), \n",
    "          # expr('left(comment,100) as Comments'), \n",
    "          col(\"namespace\"),\n",
    "          col(\"user\"),\n",
    "          when(col(\"geocoding.countryCode2\").isNotNull(), col(\"geocoding.countryCode2\")).otherwise(\"Unknown\").alias(\"CountryCode\"),\\\n",
    "          col(\"flag\"),\n",
    "          col(\"pageURL\"))\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", SILVER_CHECKPOINT_LOCATION)\n",
    "  .outputMode(\"append\")\n",
    "  .queryName('silver_stream')\n",
    "  .start(SILVER_DATA_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterDF = (kafkaCleanDF\n",
    "              .filter(col(\"geocoding.countryCode2\").contains(\"US\")) # Se ingresa a la columna \"geocoding_countryCode2\" y se seleccionan las filas con el atributo \"US\"\n",
    "              .filter(col(\"geocoding.city\").isNotNull())\n",
    "              .select(\"timestamp\", \"pageURL\", \"geocoding.countryCode2\", \"geocoding.city\") # Me quedo con los atribudos que me importan para el análisis\n",
    "              )\n",
    "display(filterDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountsDF = (\n",
    "  kafkaCleanDF\n",
    "    .filter(col(\"geocoding.country\").isNotNull())\n",
    "    .groupBy(\"geocoding.country\")    \n",
    "    .count()\n",
    ")\n",
    "\n",
    "display(CountsDF\n",
    "        .sort(\"count\", ascending=False)) # clasificaición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para detener los procesos de streaming\n",
    "for s in spark.streams.active:\n",
    "      s.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
