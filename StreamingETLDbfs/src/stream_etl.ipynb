{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, FloatType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables globales a usar en el programa\n",
    "CSV_DATA_PATH = \"dbfs:/FileStore/tables/walmart_data.csv\"\n",
    "PARTITIONS_NUMBER = 30 #numero de archivos a partir el csv de walmart \n",
    "RAW_PATH = \"dbfs:/FileStore/raw/\"\n",
    "SILVER_PATH = \"dbfs:/FileStore/silver/\"\n",
    "SILVER_CHECKPOINT_LOCATION = \"dbfs:/FileStore/silver/check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType()\\\n",
    "         .add(\"store\",IntegerType())\\\n",
    "         .add(\"Date\",StringType())\\\n",
    "         .add(\"weekly_sales\",FloatType())\\\n",
    "         .add(\"is_holiday\",IntegerType())\\\n",
    "         .add(\"temperature\",FloatType())\\\n",
    "         .add(\"fuel_price\",FloatType())\\\n",
    "         .add(\"cpi\",FloatType())\\\n",
    "         .add(\"unemployment\",FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leer data de CSV de walmart sales\n",
    "df_walmart = spark.read\\\n",
    "                  .format(\"csv\")\\\n",
    "                  .option(\"header\",True)\\\n",
    "                  .schema(schema)\\\n",
    "                  .load(CSV_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partir el conjunto de datos de walmart en n partes\n",
    "partition_data = df_walmart.repartition(PARTITIONS_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardando las n particiones del conjunto de datos en la capa raw\n",
    "partition_data.write\\\n",
    "              .format(\"parquet\")\\\n",
    "              .option(\"header\",True)\\\n",
    "              .save(RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulo el proceso en streaming que va tomando un archivo con cada ejecuci√≥n\n",
    "stream = spark.readStream\\\n",
    "              .format(\"parquet\")\\\n",
    "              .schema(schema)\\\n",
    "              .option(\"header\",True)\\\n",
    "              .option(\"ignoreLeadingWhiteSpace\",True)\\\n",
    "              .option(\"mode\",\"dropMalFormed\")\\\n",
    "              .option(\"maxFilesPerTrigger\",1)\\\n",
    "              .load(RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#escribo en la capa silver los datos tomados por medio del streaming\n",
    "stream.writeStream\\\n",
    "      .option(\"checkPointLocation\",SILVER_CHECKPOINT_LOCATION)\\\n",
    "      .format(\"parquet\")\\\n",
    "      .outputMode(\"append\")\\\n",
    "      .queryName(\"silver_stream\")\\\n",
    "      .start(SILVER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para detener los procesos de streaming\n",
    "for s in spark.streams.active:\n",
    "      s.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
